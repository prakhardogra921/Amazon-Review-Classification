from nltk.tokenize import RegexpTokenizerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import CountVectorizer#from sklearn.metrics.pairwise import cosine_similarityimport operatorfrom nltk.stem.porter import PorterStemmer#from nltk.stem import WordNetLemmatizerfrom sklearn.metrics.pairwise import euclidean_distancesstemmer = PorterStemmer()												#Porter Stemmer method for stemming the words#wordnet_lemma = WordNetLemmatizer()vectorizer = TfidfVectorizer(stop_words='english')						#TF-IDF Vectorizer to create list of feature vectorscvectorizer = CountVectorizer(ngram_range=(1, 2))                       #token_pattern=r'\b\w+\b', min_df=1tokenizer = RegexpTokenizer(r'\w+')										#tokenizer to create tokensprint("Reading data from trainhw1.txt and testdatahw1.txt")ftrain = open("trainhw1.txt")											#read training datatrain_data = ftrain.readlines()ftrain.close()ftest = open("testdatahw1.txt")											#read test datatest_data = ftest.readlines()ftest.close()fbag = open("bag_of_words.txt")											#read bag of wordsbag_of_words = fbag.read().split(" ")fbag.close()Y = [""]*(len(train_data))print("Now preprocessing the data")corpus = [""]*(len(train_data) + len(test_data))for i in range(len(train_data)):										#traverse training data and append stemmed words to corpus    new_list = []    if train_data[i][0] == "-":											#save sentiments + or - from training data        Y[i] = "-1"    else:        Y[i] = "+1"    list = tokenizer.tokenize(train_data[i][3:])    for word in list:        if word.lower() in bag_of_words:            new_list.append(stemmer.stem(word.lower()))            #new_list.append(wordnet_lemma.lemmatize(word.lower()))    corpus[i] = " ".join(new_list) + "\n"for i in range(len(test_data)):											#traverse test data and append stemmed words to corpus    new_list = []    list = tokenizer.tokenize(test_data[i])    for word in list:        if word.lower() in bag_of_words:            new_list.append(stemmer.stem(word.lower()))            #new_list.append(wordnet_lemma.lemmatize(word.lower()))    corpus[i + len(train_data)] = " ".join(new_list) + "\n"print("Creating feature vectors")#corpus2 = cvectorizer.fit_transform(corpus)#X = vectorizer.fit_transform(corpus)									#creates list of feature vectorsvectorizer= CountVectorizer(analyzer='word',tokenizer=None,preprocessor=None)bow=vectorizer.fit_transform(corpus)tfidtransformer=TfidfTransformer(norm='l2',sublinear_tf=True)bow=tfidtransformer.fit_transform(bow)Z = bow.toarray()print("Applying K Nearest Neighbour Algorithm")k = 145fresult = open("results.txt", "a+")										#file to store the results of KNN Algorithmfor i in range(len(train_data), len(train_data) + len(test_data)):		#traverse feature vectors of test data        distance = []        for j in range(len(train_data)):								#traverse feature vectors of training data            dist = euclidean_distances(Z[j].reshape(1, -1), Z[i].reshape(1, -1))            #dist = cosine_similarity(Z[j].reshape(1, -1), Z[i].reshape(1, -1))            distance.append((dist, Y[j]))        distance.sort(key=operator.itemgetter(0))						#sort the feature vectors in increasing order of Euclidean Distance        countpos = 0        countneg = 0        for j in range(k):												#to identify the majority sentiment + or -            if distance[j][1] == "+1":                countpos += 1            else:                countneg += 1        if countpos > countneg:            type = "+1"        else:            type = "-1"        fresult.write(type + "\n")fresult.close()